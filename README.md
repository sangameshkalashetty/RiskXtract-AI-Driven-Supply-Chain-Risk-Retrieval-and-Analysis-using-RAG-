# RiskXtract-AI-Driven-Supply-Chain-Risk-Retrieval-and-Analysis-using-RAG-

# Overview
This repository showcases a Retrieval-Augmented Generation (RAG) pipeline powered by Large Language Models (LLMs) and vector-based semantic search. It leverages LangChain, HuggingFace Embeddings, and ChromaDB to extract and answer questions from complex PDF documents, providing accurate, context-aware responses in real time.

# Abstract
This project implements a modular RAG system that processes unstructured documents (PDFs), transforms them into vector embeddings, and enables real-time question answering through LLMs. The workflow includes document chunking, vector storage, embedding search, and integration with Groqâ€™s Llama 3 model, delivering accurate and source-backed answers directly from document content.

# Introduction
The goal of this project is to build a complete RAG pipeline that can:

  - Load and process large PDF documents.

  - Split them into semantically meaningful chunks.

  - Convert these chunks into vector embeddings for fast retrieval.

  - Enable querying the document using natural language questions.

  - Provide real-time, source-aware answers using an LLM (Llama 3 by Groq).

# Problem Statement
Users often struggle to extract meaningful insights from lengthy, unstructured documents like whitepapers, research articles, or legal contracts. Traditional keyword search lacks contextual understanding, while manual reading is time-consuming. This RAG pipeline solves the problem by enabling semantic search and interactive Q&A on documents.

# Motivation
- **Semantic Search:** Replace keyword-based search with intelligent, context-aware retrieval.

- **Interactive QA:** Allow users to ask natural language questions and get precise, document-based answers.

- **Time Efficiency:** Save hours of manual reading by generating instant summaries or explanations.

- **Modular Design:** Easily scalable to support other document types or domains.

- **State-of-the-Art Models:** Integrates HuggingFace embeddings and LLMs for high accuracy.

# Key Features
- **RAG Pipeline:** Combines retrieval and generation steps using LangChain and Llama-3 (via Groq API).

- **PDF Parsing:** Supports unstructured PDF documents using UnstructuredFileIOLoader.

- **Document Chunking:** Uses character-based splitting to handle long documents efficiently.

- **Embedding & Storage:** Converts chunks into embeddings via HuggingFace and stores them using ChromaDB.

- **Real-Time Querying:** User can ask any question related to the document; system returns answers with sources.

- **Scalable Architecture:** Designed to work with any document type and extendable with more LLMs or retrievers.

# Tech Stack
Component	Technology Used: 
 - Document Loader LangChain Unstructured Loader
 - Text Chunking	LangChain CharacterTextSplitter
 - Embedding Generator	HuggingFace Transformers
 - Vector Database	ChromaDB
 - Language Model (LLM)	Llama 3 (Groq API)
 - Interface	Python CLI / Notebook

# Future Scope
- Add support for multiple file formats (DOCX, HTML, CSV).

- Integrate with Streamlit for a user-friendly web interface.

- Add feedback loop to improve answer accuracy.

- Extend to multi-document RAG systems and cross-referencing.
